# CSV Parser

A high-performance Rust implementation for efficiently processing large CSV files.

## Features

- **Memory Efficient**: Streaming processing maintains constant memory usage regardless of file size
- **High Performance**: Zero-cost abstractions and optimized CSV processing in Rust
- **Concurrent Processing**: Multi-threading support for maximum CPU utilization
- **Flexible Configuration**: Fine-grained control over buffer size, thread count, field selection, and more

## Building

```bash
# Release build (optimized)
cargo build --release

# Development build
cargo build
```

## Installation

```bash
# Clone the repository
git clone https://github.com/m-jingu/csvparser.git
cd csvparser

# Build the project
cargo build --release

# The binary will be available at ./target/release/csvparser
```

### Adding to PATH (Optional)

To use `csvparser` from anywhere in your terminal, add it to your PATH:

#### Linux/macOS
```bash
# Add to your shell profile (e.g., ~/.bashrc, ~/.zshrc)
echo 'export PATH="$PATH:/path/to/csvparser/target/release"' >> ~/.bashrc
source ~/.bashrc

# Or create a symlink to a directory in your PATH
sudo ln -s /path/to/csvparser/target/release/csvparser /usr/local/bin/csvparser
```

## Usage

### Basic Usage

```bash
# Read CSV from stdin and output to stdout
csvparser

# Specify input and output files
csvparser input.csv -o output.csv

# Extract specific columns (1-based indexing)
csvparser input.csv -f 1,3,5

# Adjust buffer size (default: 64KB)
csvparser input.csv --buffer-size 1048576

# Specify number of threads
csvparser input.csv --threads 4

# Show processing statistics
csvparser input.csv --stats

# Enable verbose logging
csvparser input.csv --verbose
```

### Performance Optimization

#### For Large Files (10GB+)
```bash
# Use larger buffer size
csvparser large_file.csv --buffer-size 16777216  # 16MB

# Utilize CPU cores
csvparser large_file.csv --threads 8

# Monitor performance with statistics
csvparser large_file.csv --stats --verbose
```

#### For Memory-Constrained Systems
```bash
# Use smaller buffer size
csvparser input.csv --buffer-size 32768  # 32KB
```

## Performance Characteristics

### Memory Usage
- **Base Memory**: ~2-4MB (depends on buffer size)
- **Maximum Memory**: Buffer size Ã— 2 + overhead
- **100GB Files**: Constant memory usage (independent of file size)

### Processing Speed
- **Small Files (<1GB)**: 100-500MB/second
- **Large Files (10-100GB)**: 200-800MB/second
- **CPU Utilization**: Maximum multi-core utilization

## Technical Specifications

### Architecture
- **Streaming Processing**: Never loads entire file into memory
- **Batch Processing**: Processes data in batches for efficient I/O operations
- **Zero-Copy**: Minimizes data copying wherever possible
- **Memory Mapping**: Optimized for large files on Unix-like systems

### Optimization Techniques
- **LTO (Link Time Optimization)**: Runtime optimization
- **PGO (Profile Guided Optimization)**: Profile-guided optimization support
- **SIMD Instructions**: Utilizes SIMD instructions when possible
- **Cache Optimization**: Memory access patterns optimized for CPU cache

## Error Handling

- **Robustness**: Proper error handling for malformed CSV data
- **Resilience**: Continues processing even with some record errors
- **Logging**: Detailed error information and debug data

## Limitations

- **Maximum File Size**: Theoretically unlimited (practically up to 100GB)
- **Memory Requirements**: Minimum 2MB, recommended 16MB+
- **OS Support**: Linux, macOS, Windows

## Troubleshooting

### Memory Insufficient Error
```bash
# Reduce buffer size
csvparser input.csv --buffer-size 16384
```

### Slow Processing
```bash
# Increase number of threads
csvparser input.csv --threads 8

# Increase buffer size
csvparser input.csv --buffer-size 16777216
```

### Debug Information Needed
```bash
# Enable detailed logging
RUST_LOG=debug csvparser input.csv --verbose
```
